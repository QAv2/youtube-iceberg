<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Antigravity for Scientists and Researchers â€” Iceberg Archive</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <nav><a href="../index.html">&larr; Back to index</a></nav>

    <article>
        <h1>Antigravity for Scientists and Researchers</h1>
        <div class="meta">
            <span>Channel: GDG AI for Science</span>
            <span>Published: 2026-01-22</span>
            <span>8,167 words</span>
            <span>Source: auto_caption</span>
        </div>
        <div class="tag-pills"><a href="../categories/antigravity-technology.html" class="tag-pill">Antigravity Technology</a></div>

        <div class="embed">
            <iframe width="560" height="315"
                src="https://www.youtube-nocookie.com/embed/34dS94ONuDM"
                frameborder="0" allowfullscreen></iframe>
        </div>

        <div class="transcript">
            <h2>Transcript</h2>
            <p>about how we could use such AI assisted ides to build your research and scientific pipelines. So often we see these tools being used for software development but how do we repurpose them or how do we adapt these tools for research and scientific workflows is what we&#x27;re going to discuss and um so what&#x27;s anti-gravity right so it&#x27;s an AI assisted IDE or integrated development environment similar to VS code if you have used it before and those who have not used it before I would basically show you what this looks like in a it and also the biggest uh feature of this IDE is that it&#x27;s natively agentic and has generative AI capabilities built into the platform. And this would allow you to quickly prototype your ideas and also take them to production, right? If you do the prompting really well and if you know what you&#x27;re doing. So this way we could rapidly accelerate computational and datadriven development and scientific research. which is what we&#x27;re going to see some examples of.</p>
<p>And here&#x27;s a QR code if you want to scan and download Google Anti-gravity or you could just do a simple Google search and download uh this tool if you haven&#x27;t already. And here&#x27;s what is interesting uh when it comes to the use cases of anti-gravity uh which is actually unlimited and some of what we could think of were these right. So whether you&#x27;re a researcher, student or engineer who codes, right? You could one visualize and explore your data sets, refactor or parallelize your code to execute efficiently on high performance computing environments or similar. And you could also take an existing code, break it down, understand and improve upon it. And the other one which I&#x27;m going to cover in detail today is going to be about how do you code an end toend ML research workflow all the way up to a cool dashboard.</p>
<p>Right? So where you could visualize certain things and some of the results and you could also visualize how the monitoring how the logs look like and so on and specific example within this uh idea is of building a physics informed neural network test bench with anti-gravity. So before I go further into the tutorial I&#x27;ll just briefly explain what a physics informed neural network is for those who don&#x27;t know. Um so there&#x27;s been a very beautiful review paper by professor kanyadakus from Brown University in 2021 which talks about what physics informed neural network or what physics informed machine learning is all about and you could certainly check it out to learn more about it. But uh very broadly put physics informed neural network bakes in the underlying physics of a system that you&#x27;re dealing with and or the physics that governs a particular system and you have some observed data. Then what you could do is you could blend both of these in and train a neural network to solve certain kinds of forward or inverse problems.</p>
<p>So inverse problems are those where you say that I have certain amount of data but I would like to infer what the system is. what is the underlying parameters that govern the system. Then I could blend and if I know some or full physics of the system then I could combine both of these and train a neural network to recover certain unknown or hidden variables or hidden underlying system parameters which would help me further model or understand what or what the system does or what or how the system behaves under certain circumstances. So the typical schematic is like you have inputs which are typical space-time coordinates you have a neural network backbone then it predicts a solution then you have the automatic differentiation pipeline for those who know what deep learning is and for those who don&#x27;t know just would have to worry about it the anti-gravity tool is going to take care of all of this and TensorFlow or PyTorch is going to take care of all of this for you so all you need to know is what this does right so you have a predicted solution there&#x27;s an automatic differentiation pipeline which encodes these physical laws into an objective or a loss function and then you have the observed data which you compare your predicted solution with and then you combine this datadriven loss and the physics driven draws together and optimize the neural networks parameters. So this is a brief very quick look at what physics informal networks are.</p>
<p>So now let&#x27;s get into how we could use anti-gravity to build a test bench around this. And before that if you would like to take a look at the tutorial that we have prepared u you could just scan this or maybe I could paste this link in the chat window for you. &gt;&gt; Yep. I dropped it in the chat. &gt;&gt; Okay.</p>
<p>Yeah. So you would find some examples for all these other three categories of other three ideas. So you could also test them on your own and make some changes to the prompts and see how it behaves. Yeah. So this is the tutorial if you&#x27;ve uh been able to open it.</p>
<p>So we have so the prerequisites is you it&#x27;s expected that you know some amount of coding and a motivation a strong motivation to experiment and some familiarity with what visual studio code editor looks like and if not anyway we&#x27;ll see how the ID looks like anyway so here is the first example so you could quickly try it out where you say that I I want to find out what the whether the temperature has changed in Sydney Australia let&#x27;s try and do this So I have a couple of windows open just to quickly show you things. But let&#x27;s say we open a new window gravity. so it&#x27;s taking a bit of time to open but so if you have we already have uh the ID open at your end please do try some of these prompts out and put out your comments chat window or if you have any question do let us know. So here&#x27;s the window you have [snorts] uh taskbar up and uh you have the explorer where you could open folder which is local or even you could to the chat window here on the right u you can interact with your agents and chat versions iterate in your code and this is where code gets shown for example In examples I put yeah so very brief look at it. So this is how your code will show up on your left hand side panel have uh your code that you can look at here and edit them if need be.</p>
<p>Yeah. And then on the right side you could chiterate in your project and you could choose uh whe you want to do a quick iteration and you want the agents to execute tasks very quick and think for a long time read then plan before cuting task. So you can choose between the two modes and then you also have a way to choose between your models. So depending on which one you have a good subscription of, which one offers better free tire rate limits, so you could choose the models accordingly and based on which one gives you better results. Right? So now uh yeah, so this is ready now.</p>
<p>So let&#x27;s open. So this is this is the code editor. We could also look at another feature which is the open agent manager. So where you could you would directly have uh workspaces or it&#x27;s a playground where you could interact with these agents. So let&#x27;s say now I wanted the sign and I&#x27;m fast mode.</p>
<p>Let&#x27;s use a planning and I choose 3 Pro. It takes a bit of time. I think there&#x27;s some network issue from my end. But you can look at a previous chat where I had opened up for this similar. &gt;&gt; Do you want to turn off your video re that might &gt;&gt; Yeah.</p>
<p>Yeah. So here is of I use the planning version, planning mode and it basically studies what the temperature trends look like and considering some interpretation looks at some data acquisition strategy creates a task.m MD. So I&#x27;ll come to what a task.md file is in a bit and certain approaches to figure out okay whether temperature how do I even measure certain measure the change in temperature and so on and does a detailed step-by-step process searches for historical and recent temperature data current finding current Sydney weather and comparing it retrying searches synthesizes finding into a artifact Yeah. Yeah. So now it has created a report for me which gives an executive summary saying that yes the temperature in Sydney has changed significantly over the long term showing a warming trend.</p>
<p>how a short-term weather variability means specific days like today might be still cooler than average and it goes on to prove and validate all of that. So here is also a table which talks about historical average, current status, what&#x27;s the deviation just quite scientific and rigorous in its approach. Right? So and then there are some key data points it gives and then finally go and ask it to prove it using data and code. Then it uh goes out and searches for data sets, loads them direct, searches for open openly available [clears throat] data sets where it doesn&#x27;t have to sign in and tries to download them and try then you could further try out some of these things, right? You could run some of these tests in a local environment which you have to specify because the agent is not going to uh find out on its own whether some environments exist. Whether you would have to specify certain environments in which you would like the agent to work work in.</p>
<p>And then you could ask it to do further complex tasks like give me an animated heat map or you could further ask it to visualize what the mean average uh historical average uh looks like and what the trend looks like over the last four decades and so on. So you could go dig deeper and deeper. So another example is let&#x27;s say you download some data set and you would want to build a model around it and train on this particular data set. You could do that with um anti-gravity as well. And it does a very nice uh data exploration tries to understand what the data set looks like whether it is image or text or sequences or time series and so on and then accordingly structures your model class and builds an entire code base around it.</p>
<p>So now here&#x27;s an example of that. So uh when whenever I start gravity and start interacting with the chat window and I specify what I want to do, it creates something called the artifact. So in the chat window on on your uh the chat panel on the right side. So you see these three buttons. So one talks about what is &gt;&gt; Rahul.</p>
<p>Can you just um zoom in? I think one one dot. &gt;&gt; Okay. &gt;&gt; Thanks. Yeah, this works. &gt;&gt; Yeah, good stuff.</p>
<p>Thanks. &gt;&gt; Okay. Yeah. So, yeah. So, here this is where I need to do some background effort.</p>
<p>So, I had to specify where my cond environment and a cond environment or some package manager environment is located. Then I ask it to create a cond environment for me. Then I execute the following ask the agent to execute the following commands. So it could download the data sets and then extract uh unzip and store that into some data folder. So this is a data set of human faces uh expressing five different kinds of emotions.</p>
<p>Angry, fear, happy, sad and an emotion of surprise. And it downloads and categorizes them into these folders. Once it does so now what I want is I want uh the next thing is to set the set the environment up. Once the download plan is uh data is downloaded it creates an implementation plan. Right.</p>
<p>So, so implementation plan, a task and a walk through are the key artifacts of u of this entire code development process with anti-gravity. So, it creates these artifacts on the go. So, one is called the walkthrough, one is the implementation plan. It also creates a task list. So, task list is simple to-do list of sorts.</p>
<p>It creates this on its own and tries to check this up automatically as soon as each of these tasks get done. And you could um so when you&#x27;re setting up your anti-gravity, you could also set it in a way that you would ask the agent to review all the changes it has made with the user or you could ask it to just execute it on its own automatically not asking the user&#x27;s review. Right? So you could do that and so as soon as this is created I could I could further comment on it saying that this is good to go or add further details go into granular level uh details of each of these tasks and so on. But let&#x27;s say I don&#x27;t do that. It creates these tasks.</p>
<p>It assumes and I say it&#x27;s good to go. It uses this as baseline and tries to complete the tasks one by one. So it has created a cond environment for me. It sets up the demo faces directory here. and explores the data which is all of these images and then it creates this training script and inference script and within the training script it implements tensorboard logging because I wanted to have logging so that I could monitor the loss curves as the model trains and also I could store the logs into some CSV files which I could later post-process some fashion and while the model trains I would also like to store the check checkpoints of the models at every few epochs and also store the best model checkpoint and then I would like to have periodic plotting of these uh loss history so I understand whether the model is training uh well or not and then I could also configure the training for overfitting so this is because I given my limited resources on my workstation uh which is a completely CPU based workstation that I&#x27;m I&#x27;m demoing uh this entire uh tool on.</p>
<p>So I don&#x27;t have a GPU resource. So I have to only work with smaller data samples. So the actual data sample had 100,000 or so images. I just subsampled them. Took a sample of just 130 images overall for each category and then train the model on it.</p>
<p>And I knew that since the data set is small, it&#x27;s going to overfitit on this data set. So I&#x27;m happy with it. I just want to know that my code base works and then once it once I know it works I could deploy this on any uh super cluster or run it on a GPU machine for over a larger data set right so I could do that so now I could configure the training for overfitting high epochs no early stopping right and then I run I could run enhanced training or further number of epochs then I also create an inference script for evaluation I mean all of This is planned by the agent. So I don&#x27;t explicitly uh say that I want each of these tasks done but rather what I uh do mention is this. See uh you could see that my comments are very minimal right so it says uh I just say that okay u I&#x27;ve created some qument this is where I could access uh the path the implementation had uh some I added some comments over there then then I gave a good to go signal and it was just done so it does all of this right now once it has done with the it&#x27;s done with the data exploration I just asked it to classify based on these facial expressions, write a TensorFlow model.</p>
<p>Then I didn&#x27;t want to extract the whole data set like I mentioned. So I just asked you to skip data extraction, focus on model development. And so you could see that my comments are again very minimal. And now here is where I uh mention this and accordingly the task list and the implementation plan gets modified and it is quite dynamic. So I wanted to over fit on the small data set.</p>
<p>I wanted tensorboard logging save the loss history plot loss history after every few epox have an inference script to plot predictions and so on. Right? And then it goes into gory details of how it has to do it on its own. And then so this is one artifact. So task uh list is one artifact. Then implementation plan is basically the more detailed version of this task list.</p>
<p>Right? So it looks at what are the proposed changes. It looks at training refinement, data loading, model architecture, call back implementation, inference and visualization, then verification plan and so on. And you could always see these artifacts on this small um bar above the chat box. So the third uh icon here it gives you what are the different artifacts. So implementation plan task and walk through are artifacts that the agent generates on its own and the prediction samples history confusion matrix are all images or uh artifact that are generated after running the code.</p>
<p>Right? So not exactly u the planning related artifacts per se and uh so I showed you what the implementation plan and task looked like. So now once the code is ready it creates a walk through for you to understand okay it explains what the code does to you. So kind of explains how the implementation of this advanced training and evaluation pipeline has been done. So it talks about overfitting strategy. What does it do with regard to advanced monitoring, tensorboard, CSV logging, periodic plotting and then also model checkpointing, save the best performance model, inference and visualization, created an inference for local evaluation, confusion metrics and so on.</p>
<p>So here is where it plots u the evolution of loss and accuracy. So I had only wanted to plot the loss but it did something better. It also plotted the accuracy of the model. Kindly ignore the fact that the validation accuracy or the validation loss is continuously increasing. It&#x27;s because it&#x27;s overfitting on a small data set.</p>
<p>But you could see that it compares train and val in both these plots. And u so then it gives a final summary of what the training accuracy looks like. And uh then it also tells you what files and directories are there where you could check. So it&#x27;s a very neat documentation and this helps update the overall readme of my codebase as well. So now uh so this is the code that it has generated.</p>
<p>Barely had to touch the code. I didn&#x27;t make any changes myself [snorts] anywhere in the code. And it automatically recognized that the image size was 48 + 48 and chose a batch size as appropriate for my system. And then it defines all these constants u in a very nice way follows the python pep uh styling and quite well documented and um and it&#x27;s very easy to follow the code as well. If you already know how to write Python codes for machine learning then you it&#x27;s very easier to follow.</p>
<p>The variables are named in a way that you can understand and it means what it does. Um so that&#x27;s about how the code gets generated and you could also see um what are the changes uh that have been uh made but okay since this was just one shot there were not many changes as such. So these this was a new file that was generated and this is what I had to review. So if it looks good to me I could I just go ahead and say that this is all fine. And so or I could comment on particular lines of this code saying that see I would like I could comment on this line saying that see I I would like this um import to be different or I don&#x27;t want to work with OS I would like to work with path or some other library or I could go on to say that I don&#x27;t want uh this is I want a different log directory to be used and um or let&#x27;s say I were to go here and say that I just don&#x27;t want plot history to take uh epoch as my argument.</p>
<p>I would also like to uh like this to take the save path as another argument. So it&#x27;s knows where to save uh the loss history uh plots and so on. So and accordingly the tool is also going to adapt and modify the code. Right? So you would see that what&#x27;s what&#x27;s fascinating is that I barely had to touch the code and it creates this entire folder directory has this very nice logs folder plots folder. So it nicely maps losses after every few epochs.</p>
<p>Helps me log the improvement of the model and see whether it is training well or not. Uh so far so good. &gt;&gt; So any questions so far? Yeah, couple of questions in the chat. Uh mostly just um uh logistical things. Uh a pit is asking about you know uh fundamental research and whether it can handle that.</p>
<p>And I think the answer is maybe. Yeah. Like you can give it a go. Yeah. Um, so I think where is this question? Okay.</p>
<p>I think I think I would say that this is an ID. It&#x27;s a code editor. So you could still do some amount of uh fundamental research by going onto your agent manager and providing it with certain files and context or images and asking you to do the research that you wanted to because it anyway still uses Gemini 3 Pro, right? So you could do all the research here and then convert that to code if you need because it&#x27;s it&#x27;s fundamentally a code editor. So I would say that it&#x27;s an overkill to use anti-gravity for theoretical research. You would rather be better off using something like uh Gemini itself uh to do that, right? Because this is the model that is uh being used here as well.</p>
<p>However, if you want to convert that theory theoretical research into code, then this is a platform that&#x27;s very useful. And if you want to implement something with some advanced research that you would like to do then certainly this is helpful. Now I&#x27;ll actually show an example detailed more detailed example with physics in form neural networks with what I did. So right uh so like I mentioned that I wanted to build a test bench for physics informal networks in tensorflow. So this was the prompt that I gave it.</p>
<p>I said so here&#x27;s a workbench. I just created this folder parent folder and I loaded it here and then I said so here&#x27;s a workbench for pins in tensorflow that I would like to develop ensure there are model data and test classes plan the whole framework up and let us code right so it thought for a couple of seconds I was using again planning and gemini 3 flash for this and um so you could choose between uh these modes depending on which tasks require extensive planning versus which task doesn&#x27;t require extensive planning. So if if you feel that creating a directory doesn&#x27;t does not require planning then you could just switch to fast mode like let&#x27;s say you want to install some packages you want the agent to install packages or you want the agent to review uh just process some data or create some directory I think you don&#x27;t require the planning mode you could go to fast mode and just makes things simpler and it it makes it cost effective as well because planning requires reasoning which requires a lot of iterations that the model does internally which you can avoid with this mode. Right? So that&#x27;s uh a caveat and so it thought for 2 seconds. So it creates these artifacts immediately u which is the task and the implementation plan.</p>
<p>So it creates long laundry list of tasks that need to be done and um so it and also this task list keeps evolving as I keep prompting it now and then. So it first created these five. So it creates the existing data structure implementation plan the other artifact of task.md implement the data module model module training o module and testing and then a main script and then verifies this implementation right so this plans it again I have to provide some review it asks for my review because that&#x27;s the mode I set uh this entire uh tool to so I didn&#x27;t want to automatically execute tasks without my intervention which you can also avoid if you have enough confidence on the platform. But this is just for me to understand what it does and how it does things. And so once I and and basically my comment is also useless except to say that oh this is good to go.</p>
<p>I didn&#x27;t have to touch uh a lot of these things and it was very good at executing most of the tasks. Then it implements a data module. It uh edits these uh files and it tells me what has changed and it creates a beautiful data module well documented and has these nice doc strings and everything written up and very clean very clean code and the variables are also again named in a way that I can understand what it means and uh so you could then review what are the changes so these are artifacts you can also look at what are the changes that have been made to the files. So I could look at let&#x27;s say each of the file and list down what are the seven changes that have been made to these files right and now that I have it has implemented the data module that it goes to model module solver module utils and tests makes uh creates some of these helper functions for visualization and also verify the framework components. So it automatically has a test suite as well.</p>
<p>So as soon as it creates all of these things, it does an integration testing of sorts. So test spin data generation, tests model creation, test solver step and also runs this whole thing. Then then you have the main script which is the entire training script here. So I wanted to solve the burgers equation which is a onedimensional partial differential equation. I had taken that example to make things simpler and quick and uh so it has implemented all of this and now created an end toend example using to solve Bos&#x27;s equation to demonstrate the framework.</p>
<p>It verifies the implementation and the test was taking a lot of time. So and you could always monitor what processes are running in the background with the second icon here. So one is the changes that have been made to the code. The third one was the artifacts and this is where you could take a look at what are the background processes that are running. So if you have too many processes running you could review them and close them if need be.</p>
<p>So my tests were taking a bit of time. So I asked I just closed it and then it further added a comment saying that let&#x27;s check for available environments first because there was some environment conflict also. Then we did a deep dive there. Then I created I found it found the appropriate location of my package. Then we created a cond environment over there.</p>
<p>We ran the test within this environment again. Then it updates then it updates the entire walkth through MD accordingly. And it has these instructions listed out how to set up your environment, how to use your training script and inference script and what kind of tests you could run to verify if your framework has been developed well. Then further further say that now okay now I this is where it gets interesting. So I had implemented a simple feed forward neural network as my backbone right no residual connections no attention layers and so on.</p>
<p>So I just wanted to heat things up a bit. So I said now let us try and implement a pin version with linear attention layers and residual connections in the MLP module. Implement this as a completely different model class. So then it actually thinks uh quite a bit. So then uh it so it thought for 26 seconds it revises its implementation strategy.</p>
<p>And what&#x27;s beautiful about it is that it looks at the implementation. Um it outlines the structure of these attention blocks and so on. And what it uh it also does is it also clarifies what linear attention means. So so it just tries and says okay typical attention has softmax. So I&#x27;m omitting the softmax or employing some kernel feature maps and simpler mechanisms as squeeze and exitation might be relevant too.</p>
<p>And so the request was specifically linear attention layers. Now it leans to the simplified self attention or a linear transformer layer within the context of a ResNet pin which it came up with. And so it&#x27;ll to that end it&#x27;ll implement a custom linear attention layer. Now it goes on to further interpreting what this linear attention layer is. So &gt;&gt; I I have a question.</p>
<p>Um was that uh were you using the flash model there or or a different model? Yeah, this is where I shifted to Gemini 3 Pro. Yeah. &gt;&gt; Okay. Yeah. &gt;&gt; Yeah.</p>
<p>So, wherever I needed some core research to be done, something which was vague, but I wanted the model to do a lot more thinking, I I kept the planning mode on, but I shifted to a different model. &gt;&gt; Good stuff. &gt;&gt; Yeah. And um then I I also define what the model structure looks like. You can see input embedding dense then it connects the rest block.</p>
<p>So there&#x27;s a dense plus attention layer then into n hidden layers then gives out the output. So it looks at it goes even one step further to say okay I want an efficient linear attention mechanism that is because standard one is order n2 which is problematic. So it&#x27;s going to look at a linear version which is order n and uh then it also looks at revising the implementation strategy then updates the task md file. Then it says okay now verify designing attention reset pin implementing these layers create an example verify attention model improve documents refactor utters for generic plotting and so on. Yeah, then I had some comments to make like I wanted the input attention feed into successive hidden layers and restrict the residual connections only within the hidden layers u and not connected from the input to the output.</p>
<p>Then I again thought for a longer time then made further changes to these input features and uh and the best part is you can see what the reasoning is behind all of this. So if you find some of them to be logically flawed, you could go back and again review it and further add comments making uh in a very specific way. Then it planned this center attention module. It refineses it and creates this beautiful class called attention restnet pin which inherits from TF uh TensorFlow model class. And um so now it has defined an input attention block in another module called layers and it calls that here.</p>
<p>And then this is the layers module where it defines the input attention block. But before that it attend uh defines a custom linear attention layer. The key value computation and the entire numerator denominator the entire attention is computed here. And then you have the input attention block where it defines your dense layers and also the attention block which gets projected and then summed up which is quite a complex operation which is way different from what a typical pin looks like. It&#x27;s just a feed forward uh dense uh neural network.</p>
<p>So you just have the inputs [snorts] feeding forward. No residual connections, nothing. Compared to that, this is a slightly more complex version and um it implements this quite well and it even verifies it runs well and um then goes further further further than it verifies this attention module within my environment. Once it has implemented it also updates the walkth through. Now I said that I wanted a completeness of documentation not leaving out any of the changes that have been made.</p>
<p>I wanted modularity of the code. I wanted this test bench to be reusable and extensible to other PDEs. So what that means is okay the model is uh developed the data is developed right now in the main file you would see that this is my training script. So I have to specify what my system parameters are, what my domain bounds are, what the data handler and um I generate the entire input uh data set and then I get the exact boundary condition values. If you know how you solve a typical PTE then you need boundary conditions or initial conditions to solve uh these PDEs.</p>
<p>If you were and if you were to then move on to create the pin model, you specify what your input dimensions are, output dimensions are, what are the hidden layers, activation function that you would like to use and then it defines another function. So given that the example is of a Burgger&#x27;s equation, it creates this function here defines what this B PD is. And this is where the magic happens in pin. So you have automatic differentiation. It computes these higher order derivatives and then computes something called the residual or the physics residual which is then used to compute your physics loss.</p>
<p>So which is the first derivative of time then a convection term basically the variable multiplied with the first derivative in space then system parameter then the second derivative in space. So, or the diffusion term like we call it. &gt;&gt; I I think it&#x27;s important to call out that whilst it&#x27;s amazing you didn&#x27;t have to write any of this or know how to write it. Uh it helped a lot that you knew sort of what you were looking for like and also you knew just a little bit of uh uh coding in order to like validate it. I think that still goes a long way.</p>
<p>&gt;&gt; Yes. Yes. Yes. Absolutely. And I think I think if you if you are very new to coding even then I think this is a tool that will help you because uh it does given that it&#x27;s an agentic setup the agents ensure that whatever code it develops it also validates and verifies them and you would which hence ensures that you have minimal intervention.</p>
<p>So it&#x27;s important that you have some domain expertise where especially if you are from the research and science background then it helps to have certain domain expertise to know okay this is how a PD should look and as long as the function is written in a way that it represents the PD that I want to solve I&#x27;m happy and the agent takes care of ensuring that this entire code works and um then you define the solver so this is the uh part where it generalized for me because I wanted to extend this entire framework beyond just one kind of a PTE. So, so this is where I can mention feed this function as a parameter to this entire solver or the model class saying that okay this is a physics uh system that I&#x27;m concerned with. Let&#x27;s say if I were to go to the solver module. So this pin solver is inherited from the sol is is called imported from the solver module. And here this is where I can specify the PD function that I want to consider.</p>
<p>This is where it has made some modification, generalized it and now this is extensible to any other PD that I would like to work with and all pin for. Then it polishes the entire documentation, updates the utils file. This utils file has a lot of these plotting functions. So it plots the solution, one plots the loss histories, one plots the error analysis, so on. And then uh I the one thing was it since my instructions were not very clear.</p>
<p>So it just plots the loss function but never saved the loss u file loss history to any file. So I wanted that to be done and also plot them as PD uh PNGs whenever possible. And so that was done. And then you could see the logs here and also the loss curve. So one is the training loss.</p>
<p>The blue line is the training loss. Orange is the physics loss. Green is the boundary loss. It&#x27;s just for first 10 epochs. So you wouldn&#x27;t see it converging yet.</p>
<p>And um then it lo it implements better plotting, better logging, fixing the Python path. Then I further uh give instructions on I want model checkpointing, I want best checkpoints, tensorboard logging. So the logging was done into a CSV file earlier, but now I want tensorboard logging. So I can visualize it in a dashboard. Then u the plotted solution should also be compared with uh with a true solutions and so on and so I give it further stronger instruction saying the thing through so I want space this pointwise residuals time in terms of time I want spatially average temporary evolving errors to be evaluated and so on right because that&#x27;s what my domain expertise is and I would like to bring that and ensure that the agent does a good job in baking in these domain specific considerations and then uh once this is done uh it says it&#x27;s all done good to go implement these tasks now I wanted to do go further uh update the documentation now all this all said and done I think we don&#x27;t want the research workflows to a only as a back-end framework that we run behind the screens and then we have the final results post-processed and stored somewhere and presented in a paper or a slide deck right it would be nice to have a dynamic dashboard where you could launch these runs monitor these logs and also be able to visualize some of these results and that&#x27;s where I wanted to have a front end around this entire framework so I could navigate test runs launch training runs depending on the availability of resources this and also I wanted to look at some of the results.</p>
<p>Then it again planned built the entire streamlit dashboard for me had to barely do anything except for add some comments here and there with regard to the cond environment and saw whether it runs fine or not. So here I&#x27;ve launched the [snorts] dashboard here. So we can take a look. So let me know if the screen is still visible. &gt;&gt; Uh, yep.</p>
<p>Going to your local host. &gt;&gt; Yeah. Okay. So maybe I have to just launch it again. Oh yeah.</p>
<p>&gt;&gt; So this is the beauty of this entire platform, right? So you could take anything from an idea all the way up to a prototype or even some sort of a minimal viable product and which is also researchdriven, datadriven and bakes in a lot of your domain expertise and u that&#x27;s the beauty of this entire framework. So while this gets launched, I would like you guys to take a look at the other examples here. So one is you could update some existing code and ask it to explain what this code does. For example, I had another code which I wanted to understand and um so this does some sort of download some sequencing data gene sequencing data and I this was a code it loads some of this data set and processes those sequences but I wanted to understand understand what this code us. So I this is simple question.</p>
<p>So I just asked what this code does. Then it thought for a couple of seconds. I used Gemini 3 flash over there and it explains and I and again it was not in mode and use Gemini 3 flash and it still does a great job in explaining what this code does. And then summary of usage also explains the usage. Now I said okay this is a serial version of the code.</p>
<p>What if I wanted to parallelize it and I just said that I don&#x27;t want to validate or execute it yet but just create the new file and explain the differences in the code. So it creates this and then has an implementation plan implements it. Then I can go through what the implementation plan looks like. explains what each of these functions do in order to parallelize my code, right? So, and it does a great job at then finally um creating a parallel version of the code and I run a couple of benchmarks on my local system. I did find with that again Rahul uh yeah going back to having some domain knowledge I was really trying to get it to be embarrassingly parallel but it just wouldn&#x27;t do that.</p>
<p>they kept trying to like implement MBI and all all these like hectic things and I&#x27;m like no I just want to want it to be embarrassingly parallel when uh when you create this file and but I had to be really explicit about that. So unless you know what what that means &gt;&gt; it won&#x27;t just do it out of the box. &gt;&gt; Yeah. Yeah. So I even here I think I didn&#x27;t give it specific uh except for the fact that my system is a CPU based system at least the one that I&#x27;m using for the demo.</p>
<p>So I want to constrain it within that resource. So that&#x27;s the only thing I specified. But it does a good job in terms of determining what kind of parallelization to do, how many sequences to even use for benchmarking. C &gt;&gt; can you go back to full screen as well, Ro? &gt;&gt; Thanks. &gt;&gt; Um, we&#x27;ve only got a couple minutes left.</p>
<p>Uh do you want to just show the the settings in the bottom right hand corner? So keep people can keep in mind you very few settings but there everything you do during setup you can change and &gt;&gt; and you can change things as you go. So uh this is autofix linds helps uh this helps what are the lint errors created by its edits and also it may fix it without explicit user prompting and however this uses agent tool usage then it also has an option to auto execution which is where I&#x27;ve set it up as request review you could always say that the agent can proceed without your review if you uh if you&#x27;re confident about certain domains where you know it does a great job and then you have the review policy. So this is like you could always collaborate with the agent or ask the agent to be as autonomous as possible. Right? So you could choose between these two always proceed or request review. Then you could further manage some custom customizations rules add some rules add some workflows.</p>
<p>But this is a slightly more advanced. I did I did have it ask me one time if it could go up up a directory like out of the directory it was current currently in. I was like oh oh jeez like that&#x27;s that&#x27;s a bit scary but um you know I said no because it wanted to find more context. &gt;&gt; So I was it was interesting that it knew you know where it was but it wanted to &gt;&gt; escape out of the uh the environment. So keep that in mind.</p>
<p>&gt;&gt; Yeah. So, so that&#x27;s one about uh the agent automatically navigating certain directories. The other thing is if you were to allow it browser access, then sometimes if you were to use a not so good model or low-end model and you were to use the fast mode, it tends to open a lot of these uh uh tabs which again overburdens your system and if it&#x27;s not able to find the exact result that it&#x27;s looking for. However, if it finds the result uh that it&#x27;s looking for, finds the thing that it&#x27;s looking for, it&#x27;s it&#x27;s fine. But otherwise, it [snorts] keeps going into this uh recurring recurring loop.</p>
<p>And um so that&#x27;s a bit dangerous as well. Uh so we have to be careful of uh the browser access and even there I&#x27;ve asked the agent to review and collaborate with me rather than automatically u navigating the browser. So you could u further set some of these things up and you have advanced settings. I would greatly encourage all of all of you to explore uh what these features are and also go through the documentation of anti-gravity to understand them. And there are some AI shortcuts that you can set up or you could uh look at this.</p>
<p>This is just open command and this one opens the agent that you&#x27;re working with. So yeah and you could also rate. So this rating helps uh calibrate the model responses as per your liking and your preferences as well. So then once I created all of this had the dashboard ready I also asked it to create a tutorial uh of this which is what you see here right so u that&#x27;s the best part about it and um let&#x27;s look at yeah this is the dashboard that the model created So we can launch training here. I could choose what kind of model architecture I want.</p>
<p>I could also name create a new name for the output directory. I could change the learning rate in orders of 10 and uh then I could choose the number of epochs in order of 100 and then I could monitor uh see where my log files are stored and then I can analyze some of these results. It tells me what the final loss is and so on. And it also tells me what the system usage is, right? Which is quite good for a proof of concept. &gt;&gt; Good stuff, Rahul.</p>
<p>Um, all right, we got one minute left. You want to your final thoughts? Yeah. So I would like to end saying that it&#x27;s always better to use this uh IDE uh more as architectural prompting perspective rather than instructional prompting like do this do that but rather ask it to initially build a system that functions like this and so on and so you function more as a project manager and uh more like an agent orchestration engineer right so then you could also use Gemini let&#x27;s say you want to build something ask Gemini to give you a prompt um structured around your question or a particular topic. And you could even go one step further and ask you to generate a complete product requirement document and convert this to a markdown file and so on and then copy this into your uh agent manager or even your chat window section by section and ask it to implement accordingly. And uh yeah and experiment see what works for you and also share your feedback with us.</p>
<p>We will have um postevent uh surveys sent out to you. Right. So thank you so much and look stay tuned for upcoming events. We have one event scheduled on Feb 9th and then March 5th. And u do follow us and uh join EG to stay informed about our all our community events future.</p>
<p>Awesome. Thanks so much Raul and yeah hope to see you all again at the next one. Thanks.</p>
        </div>
    </article>

    <footer>
        <p><a href="../index.html">&larr; Back to index</a></p>
    </footer>
</body>
</html>